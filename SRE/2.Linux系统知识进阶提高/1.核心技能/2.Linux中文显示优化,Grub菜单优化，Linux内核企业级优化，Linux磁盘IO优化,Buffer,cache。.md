### Linux中文显示优化

### Linux_Grub菜单优化

### Linux内核企业级优化

##### 安装BBR；升级内核到最新版本/5.0.2

```bash
载入公钥和yum源
    rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
    rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm
    yum install -y yum-plugin-fastestmirror
    yum --enablerepo=elrepo-kernel install kernel-ml kernel-ml-devel -y
将kernel-ml 选为第一启动
    grub2-set-default 0
    reboot
重启后，通过 uname -a 查看内核是否切换到最新版
    echo "net.core.default_qdisc=fq" >> /etc/sysctl.conf
    echo "net.ipv4.tcp_congestion_control=bbr" >> /etc/sysctl.conf
    sysctl -p    保存配置文件
开启BBR
    sysctl net.ipv4.tcp_available_congestion_control
    sysctl net.ipv4.tcp_congestion_control
查看是否开启了bbr
 	lsmod | grep bbr    
	tcp_bbr         20480  6 
123456789101112131415161718
```

##### 内核参数解析

```bash
# vim /etc/sysctl.conf
net.ipv4.ip_forward = 0                        # 表示开启路由功能，0是关闭，1是开启
net.ipv4.conf.all.rp_filter = 1                # 加强入站过滤和出站过滤
net.ipv4.conf.default.rp_filter = 1            # 开启反向路径过滤
net.ipv4.conf.default.accept_source_route = 0  # 处理无源路由的包
kernel.sysrq = 0                               # 控制系统调试内核的功能要求
kernel.core_uses_pid = 1                       # 用于调试多线程应用程序
net.bridge.bridge-nf-call-ip6tables = 0
net.bridge.bridge-nf-call-iptables = 0
net.bridge.bridge-nf-call-arptables = 0
kernel.msgmnb = 65536            # 所有在消息队列中的消息总和的最大值(msgmnb=64k)
kernel.msgmax = 65536            # 指定内核中消息队列中消息的最大值(msgmax=64k)
kernel.shmmax = 68719476736      #是核心参数中最重要的参数之一，用于定义单个共享内存段的最大值，64位linux系统：可取的最大值为物理内存值-1byte，建议值为多于物理内存的一半，一般取值大于SGA_MAX_SIZE即可，可以取物理内存-1byte。例如，如果为64GB物理内存，可取64*1024*1024*1024-1=68719476735
kernel.shmall = 4294967296       # 该参数控制可以使用的共享内存的总页数。Linux共享内存页大小为4KB,共享内存段的大小都是共享内存页大小的整数倍。一个共享内存段的最大大小是 16G，那么需要共享内存页数是16GB/4KB=16777216KB /4KB=4194304（页），也就是64Bit系统下16GB物理内存，设置kernel.shmall = 4194304才符合要求(几乎是原来设置2097152的两倍)
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv4.neigh.default.gc_stale_time = 120
net.ipv4.conf.default.arp_announce = 2
net.ipv4.conf.all.arp_announce = 2
net.ipv4.conf.lo.arp_announce = 2

    ###内存资源使用相关设定
net.core.wmem_default = 8388608     # 为TCP socket预留用于发送缓冲的内存默认值（单位：字节）一般要低于net.core.wmem_default的值。默认值为16384(16K)
net.core.rmem_default = 8388608     # 为TCP socket预留用于接收缓冲的内存默认值（单位：字节）
net.core.rmem_max = 16777216        # 为TCP socket预留用于接收缓冲的内存最大值（单位：字节）
net.core.wmem_max = 16777216        # TCPsocket预留用于发送缓冲的内存最大值（单位：字节）
net.ipv4.tcp_rmem = 4096 65536 16777216        # 接收窗口的最大大小
net.ipv4.tcp_wmem = 4096 65536 16777216        # 默认的接收窗口大小，默认值为4096(4K)
net.ipv4.tcp_mem = 8388608 8388608 8388608     # net.ipv4.tcp_mem[0]:低于此值，TCP 没有内存压力。net.ipv4.tcp_mem[1]:在此值下，进入内存压力阶段。net.ipv4.tcp_mem[2]:高于此值，TCP 拒绝分配socket。上述内存单位是页，而不是字节

    ##应对DDOS攻击,TCP连接建立设置
net.ipv4.tcp_syncookies = 1            # 只有在内核编译时选择了CONFIG_SYNCOOKIES时才会发生作用。当出现syn等候队列出现溢出时象对方发送syncookies。目的是为了防止syn flood攻击。
net.ipv4.tcp_synack_retries = 1        # 为了打开对端的连接，内核需要发送一个SYN 并附带一个回应前面一个SYN 的ACK。也就是所谓三次握手中的第二次握手。这个设置决定了内核放弃连接之前发送SYN+ACK 包的数量。减少系统SYN连接重试次数，为了打开对端的连接，内核需要发送一个SYN并附带一个回应前面一个SYN的ACK。
net.ipv4.tcp_syn_retries = 1           #在内核放弃建立连接之前发送SYN 包的数量
net.ipv4.tcp_max_syn_backlog = 262144  #表示SYN队列的长度，默认为1024，加大队列长度为262144，可以容纳更多等待连接的网络连接数。

    ##应对timewait过高,TCP连接断开设置
net.ipv4.tcp_max_tw_buckets = 10000     # timewait 的数量，默认是180000。表示系统同时保持TIME_WAIT的最大数量，如果超过这个数字，TIME_WAIT将立刻被清除并打印警告信息。
net.ipv4.tcp_tw_recycle = 1    # 表示开启TCP连接中TIME-WAIT sockets的快速收回功能，默认为 0 ，表示关闭。
net.ipv4.tcp_tw_reuse = 1      # 表示开启重用。允许将TIME-WAIT sockets重新用于新的 TCP 连接，默认为 0 表示关闭
net.ipv4.tcp_timestamps = 0    # 时间戳可以避免序列号的卷绕
net.ipv4.tcp_fin_timeout = 5   # 表示如果套接字由本端要求关闭，这个参数决定了它保持在FIN-WAIT-2 状态的时间。对端可以出错并永远不关闭连接，甚至意外当机。缺省值是 60 秒。2.2 内核的通常值是 180 秒，3你可以按这个设置，但要记住的是，即使你的机器是一个轻载的 WEB 服务器，也有因为大量的死套接字而内存溢出的风险，FIN- WAIT-2 的危险性比 FIN-WAIT-1 要小，因为它最多只能吃掉1.5K 内存，但是它们的生存期长些。
net.ipv4.ip_local_port_range = 4000 65000    # 表示用于向外连接的端口范围

    ###TCP keepalived 连接保鲜设置
net.ipv4.tcp_keepalive_time = 1200    #表示当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时，改为20分钟
net.ipv4.tcp_keepalive_intvl = 15     #当探测没有确认时，重新发送探测的频度。缺省是75秒。
net.ipv4.tcp_keepalive_probes = 5     # 在认定连接失效之前，发送多少个TCP的keepalive探测包。缺省值是9。这个值乘以tcp_keepalive_intvl之后决定了，一个连接发送了keepalive之后可以有多少时间没有回应

    ###其他TCP相关调节
net.core.somaxconn = 262144            # listen(函数)的默认参数,挂起请求的最大数量限制。web 应用中listen 函数的backlog 默认会给我们内核参数的net.core.somaxconn 限制到128，而nginx 定义的NGX_LISTEN_BACKLOG 默认为511，所以有必要调整这个值
net.core.netdev_max_backlog = 262144   # 每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目
net.ipv4.tcp_max_orphans = 3276800
net.ipv4.tcp_sack = 1            # 有选择的应答，1表示yes ，0表示no
net.ipv4.tcp_window_scaling = 1  # 支持更大的TCP窗口。 如果TCP窗口最大超过65535（64K）， 必须设置该数值为1

    ###文件系统事件监控机制
fs.inotify.max_user_instances = 1048576    # 每个用户能启动的inotify最大实例数
fs.inotify.max_user_watches = 1048576      # 每个实例最大的监控数（inode数量）
fs.aio-max-nr= 1048576        # aio最大值
fs.file-max = 1048575         # 文件描述符的最大值
fs.nr_open = 9999999          # 单个进程允许的最大 fd 数量
fs.file-max = 9999999         # linux 内核允许的最大 fd 数量

    ###VM内存分配策略
vm.min_free_kbytes = 65536    # 保留内存的最低值
vm.overcommit_memory = 1      # "0，1，2" 
0： (默认)表示内核将检查是否有足够的可用内存供应用进程使用；如果有足够的可用内存，内存申请允许；否则，内存申请失败，并把错误返回给应用进程。0 即是启发式的overcommitting handle,会尽量减少swap的使用,root可以分配比一般用户略多的内存
1： 表示内核允许分配所有的物理内存，而不管当前的内存状态如何，允许超过CommitLimit，直至内存用完为止。在数据库服务器上不建议设置为1，从而尽量避免使用swap.
2： 表示不允许超过CommitLimit值

    ###开启BBR
net.core.default_qdisc = fq                 # 启用 BBR 拥塞控制算法（需内核支持）
net.ipv4.tcp_congestion_control = bbr       # 启用 BBR 拥塞控制算法（需内核支持）
1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374
```

##### 配置文件

```bash
# vim /etc/sysctl.conf
net.ipv4.ip_forward = 0        
net.ipv4.conf.all.rp_filter = 1
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.default.accept_source_route = 0
kernel.sysrq = 0
kernel.core_uses_pid = 1
net.bridge.bridge-nf-call-ip6tables = 0
net.bridge.bridge-nf-call-iptables = 0
net.bridge.bridge-nf-call-arptables = 0
kernel.msgmnb = 65536
kernel.msgmax = 65536
kernel.shmmax = 68719476736
kernel.shmall = 4294967296
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv4.neigh.default.gc_stale_time = 120
net.ipv4.conf.default.arp_announce = 2
net.ipv4.conf.all.arp_announce = 2
net.ipv4.conf.lo.arp_announce = 2

###内存资源使用相关设定
net.core.wmem_default = 8388608 
net.core.rmem_default = 8388608 
net.core.rmem_max = 16777216 
net.core.wmem_max = 16777216 
net.ipv4.tcp_rmem = 4096 65536 16777216
net.ipv4.tcp_wmem = 4096 65536 16777216     
net.ipv4.tcp_mem = 8388608 8388608 8388608

##应对DDOS攻击,TCP连接建立设置
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_synack_retries = 1 
net.ipv4.tcp_syn_retries = 1 
net.ipv4.tcp_max_syn_backlog = 262144

##应对timewait过高,TCP连接断开设置
net.ipv4.tcp_max_tw_buckets = 10000 
net.ipv4.tcp_tw_recycle = 1 
net.ipv4.tcp_tw_reuse = 1 
net.ipv4.tcp_timestamps = 0 
net.ipv4.tcp_fin_timeout = 5
net.ipv4.ip_local_port_range = 4000 65000

###TCP keepalived 连接保鲜设置
net.ipv4.tcp_keepalive_time = 1200
net.ipv4.tcp_keepalive_intvl = 15
net.ipv4.tcp_keepalive_probes = 5

###其他TCP相关调节
net.core.somaxconn = 262144
net.core.netdev_max_backlog = 262144  
net.ipv4.tcp_max_orphans = 3276800
net.ipv4.tcp_sack = 1
net.ipv4.tcp_window_scaling = 1

###文件系统事件监控机制
fs.inotify.max_user_instances = 1048576        
fs.inotify.max_user_watches = 1048576
fs.aio-max-nr= 1048576
fs.file-max = 1048575
fs.nr_open = 9999999
fs.file-max = 9999999

###VM内存分配策略
vm.overcommit_memory = 1
vm.min_free_kbytes = 65536

reboot        # 重启机器
sysctl -p     # 保存配置文件使其生效
```

### Linux磁盘IO优化，Buffer，cache

在Linux系统中，磁盘I/O优化是提升系统性能的重要手段之一。Buffer和Cache在其中起着关键作用。下面是一些优化Buffer和Cache的方法及其原理：

#### 1. Buffer和Cache的基本概念

- **Buffer**：主要用于将数据从内存写入磁盘之前的暂存区。它确保数据在写入磁盘之前进行一定的批处理，从而提高写入效率。
- **Cache**：用于缓存从磁盘读取的数据，这样下次访问相同数据时可以直接从内存读取，减少磁盘I/O操作。

#### 2. 使用`sysctl`优化参数

通过调整内核参数，可以优化Buffer和Cache的行为。常用的参数包括：

- `vm.dirty_background_ratio`：内核将开始写入脏数据到磁盘的阈值，以物理内存的百分比表示。减少此值可以让内核更频繁地将脏数据写入磁盘，防止突发性的大量写入。
- `vm.dirty_ratio`：允许的最大脏数据量，以物理内存的百分比表示。增大此值可以增加内存中的脏数据量，从而减少频繁的磁盘I/O，但在数据丢失时可能会丢失更多数据。
- `vm.swappiness`：控制系统交换分区的倾向，值越低表示越不愿意使用交换分区，值越高表示更积极地使用交换分区。

示例：

```bash
bash复制代码sudo sysctl -w vm.dirty_background_ratio=10
sudo sysctl -w vm.dirty_ratio=20
sudo sysctl -w vm.swappiness=10
```

#### 3. 调整I/O调度器

不同的I/O调度器有不同的性能特性，选择合适的I/O调度器可以显著提高系统性能。常见的I/O调度器包括：

- `cfq`（完全公平队列）：适合桌面系统。
- `deadline`：适合需要低延迟的场景。
- `noop`：适合SSD等设备，因为其自身已经有较好的I/O调度机制。

查看当前调度器：

```bash
bash
复制代码
cat /sys/block/sda/queue/scheduler
```

更改调度器：

```bash
bash
复制代码
echo deadline | sudo tee /sys/block/sda/queue/scheduler
```

#### 4. 使用`hdparm`工具优化磁盘

`hdparm`可以用于优化磁盘的各种参数，例如启用磁盘的DMA模式等。

启用DMA模式：

```bash
bash
复制代码
sudo hdparm -d1 /dev/sda
```

查看磁盘性能：

```bash
bash
复制代码
sudo hdparm -Tt /dev/sda
```

#### 5. 文件系统优化

选择合适的文件系统和优化文件系统参数可以提高I/O性能。例如，ext4文件系统可以通过以下方式进行优化：

- 启用`noatime`选项，避免每次读取文件时更新访问时间。
- 使用合适的块大小和inode数量。

示例：

```bash
bash
复制代码
sudo mount -o noatime,defaults /dev/sda1 /mnt
```

#### 6. 监控和调整

定期监控系统的I/O性能，使用工具如`iostat`、`vmstat`、`dstat`等，找出瓶颈并进行相应调整。

示例：

```bash
bash复制代码iostat -x 1
vmstat 1
dstat
```

#### 总结

优化Linux系统中的磁盘I/O涉及多个方面，包括调整内核参数、选择合适的I/O调度器、优化文件系统参数等。通过合理的配置和优化，可以显著提高系统的I/O性能和整体响应速度。